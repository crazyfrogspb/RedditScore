

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>CrazyTokenizer &mdash; RedditScore 0.4.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Modelling" href="modelling.html" />
    <link rel="prev" title="Data collection" href="data_collection.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> RedditScore
          

          
          </a>

          
            
            
              <div class="version">
                0.4.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">RedditScore Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_collection.html">Data Collection</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tokenizing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tokenizer-description">Tokenizer description</a></li>
<li class="toctree-l2"><a class="reference internal" href="#initializing">Initializing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#features">Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lowercasing-and-all-caps">Lowercasing and all caps</a></li>
<li class="toctree-l3"><a class="reference internal" href="#normalizing">Normalizing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ignoring-quotes">Ignoring quotes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#removing-stop-words">Removing stop words</a></li>
<li class="toctree-l3"><a class="reference internal" href="#word-stemming-and-lemmatizing">Word stemming and lemmatizing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#removing-punctuation-and-linebreaks">Removing punctuation and linebreaks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decontracting">Decontracting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dealing-with-hashtags">Dealing with hashtags</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dealing-with-special-tokens">Dealing with special tokens</a></li>
<li class="toctree-l3"><a class="reference internal" href="#urls">URLs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#extra-patterns-and-keeping-untokenized">Extra patterns and keeping untokenized</a></li>
<li class="toctree-l3"><a class="reference internal" href="#converting-whitespaces-to-underscores">Converting whitespaces to underscores</a></li>
<li class="toctree-l3"><a class="reference internal" href="#removing-non-unicode-characters">Removing non-unicode characters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#emojis">Emojis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unicode-and-hex-characters">Unicode and hex characters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="modelling.html">Modelling</a></li>
<li class="toctree-l1"><a class="reference internal" href="apis/api_main.html">API Documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">RedditScore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>CrazyTokenizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/tokenizing.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="crazytokenizer">
<h1>CrazyTokenizer<a class="headerlink" href="#crazytokenizer" title="Permalink to this headline">¬∂</a></h1>
<div class="section" id="tokenizer-description">
<h2>Tokenizer description<a class="headerlink" href="#tokenizer-description" title="Permalink to this headline">¬∂</a></h2>
<p>CrazyTokenizer is a part of <a class="reference external" href="https://github.com/crazyfrogspb/RedditScore">RedditScore project</a>.
It‚Äôs a tokenizer - tool for splitting strings of text into tokens. Tokens can
then be used as input for a variety of machine learning models.
CrazyTokenizer was developed specifically for tokenizing Reddit comments and
tweets, and it includes many features toNote deal with these types of documents.
Of course, feel free to use for any other kind of text data as well.</p>
<p>CrazyTokenizer is based on the amazing <a class="reference external" href="https://spacy.io/">spaCY NLP framework</a>.
Make sure to check it out!</p>
</div>
<div class="section" id="initializing">
<h2>Initializing<a class="headerlink" href="#initializing" title="Permalink to this headline">¬∂</a></h2>
<p>To import and to initialize an instance of CrazyTokenizer with the default
preprocessing options, do the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">redditscore.tokenizer</span> <span class="k">import</span> <span class="n">CrazyTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">()</span>
</pre></div>
</div>
<p>Now you can start tokenizing!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;@crazyfrogspb Hey,dude, have you heard that&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="s2">&quot; https://github.com/crazyfrogspb/RedditScore is the best Python library?&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;@crazyfrogspb&#39;, &#39;hey&#39;, &#39;dude&#39;, &#39;have&#39;, &#39;you&#39;, &#39;heard&#39;, &#39;that&#39;,</span>
<span class="go">&#39;https://github.com/crazyfrogspb/RedditScore&#39;, &#39;is&#39;, &#39;the&#39;, &#39;best&#39;, &#39;python&#39;, &#39;library&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="lowercasing-and-all-caps">
<h3>Lowercasing and all caps<a class="headerlink" href="#lowercasing-and-all-caps" title="Permalink to this headline">¬∂</a></h3>
<p>For many text classification problems, keeping capital letters only
introduces unnecessary noise. Setting <code class="docutils literal notranslate"><span class="pre">lowercase=True</span></code> (True by default)
will lowercase all words in your documents.</p>
<p>Sometimes you want to keep things typed in all caps (e.g., abbreviations).
Setting <code class="docutils literal notranslate"><span class="pre">keepcaps=True</span></code> will do exactly that (default is False).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">keepcaps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;Moscow is the capital of RUSSIA!&#39;</span><span class="p">)</span>
<span class="go">[&#39;moscow&#39;, &#39;is&#39;, &#39;the&#39;, &#39;capital&#39;, &#39;of&#39;, &#39;RUSSIA&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="normalizing">
<h3>Normalizing<a class="headerlink" href="#normalizing" title="Permalink to this headline">¬∂</a></h3>
<p>Typing like thiiiis is amaaaaazing! However, in terms of text classification
<em>amaaaaazing</em> is probably not too different from <em>amaaaazing</em>. CrazyTokenizer
can normalize sequences of repeated characters for you. Just set <code class="docutils literal notranslate"><span class="pre">normalize=n</span></code>,
where <em>n</em> is the number of characters you want to keep. Default value is 3.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;GOOOOOOOOO Patriots!!!!&#39;</span><span class="p">)</span>
<span class="go">[&#39;gooo&#39;, &#39;patriots&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="ignoring-quotes">
<h3>Ignoring quotes<a class="headerlink" href="#ignoring-quotes" title="Permalink to this headline">¬∂</a></h3>
<p>People often quote other comments or tweets, but it doesn‚Äôt mean that they
endorse the original message. Removing the content of the quotes can help
you to get rid of that. Just set <code class="docutils literal notranslate"><span class="pre">ignorequotes=True</span></code> (False by deafult).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">ignorequotes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;And then she said: &quot;I voted for Donald Trump&quot;&#39;</span><span class="p">)</span>
<span class="go">[&#39;and&#39;, &#39;then&#39;, &#39;she&#39;, &#39;said&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="removing-stop-words">
<h3>Removing stop words<a class="headerlink" href="#removing-stop-words" title="Permalink to this headline">¬∂</a></h3>
<p>Removing stop words can sometimes significantly boost performance of your
classifier. CrazyTokenizer gives you a few options to remove stop words:</p>
<blockquote>
<div><ul class="simple">
<li>Using built-in list of the english stop words (<code class="docutils literal notranslate"><span class="pre">ignorestopwords=True</span></code>)</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">ignorestopwords</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;PhD life is great: eat, work, and sleep&#39;</span><span class="p">)</span>
<span class="go">[&#39;phd&#39;, &#39;life&#39;, &#39;great&#39;, &#39;eat&#39;, &#39;work&#39;, &#39;sleep&#39;]</span>
</pre></div>
</div>
<ul class="simple">
<li>Using NLTK lists of stop words. Just pass the name of the language
of your documents to the <code class="docutils literal notranslate"><span class="pre">ignorestopwords</span></code> parameter.</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">ignorestopwords</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="go"># You might have to run nltk.download(&#39;stopwords&#39;) first</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;PhD life is great: eat, work, and sleep&#39;</span><span class="p">)</span>
<span class="go">[&#39;phd&#39;, &#39;life&#39;, &#39;great&#39;, &#39;eat&#39;, &#39;work&#39;, &#39;sleep&#39;]</span>
</pre></div>
</div>
<ul class="simple">
<li>Alternatively, you can supply your own custom list of the stop words. Letter case doesn‚Äôt matter.</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">ignorestopwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Vladimir&#39;</span><span class="p">,</span> <span class="s2">&quot;Putin&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;The best leader in the world is Vladimir Putin&quot;</span><span class="p">)</span>
<span class="go">[&#39;the&#39;, &#39;best&#39;, &#39;leader&#39;, &#39;in&#39;, &#39;the&#39;, &#39;world&#39;, &#39;is&#39;]</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="word-stemming-and-lemmatizing">
<h3>Word stemming and lemmatizing<a class="headerlink" href="#word-stemming-and-lemmatizing" title="Permalink to this headline">¬∂</a></h3>
<p>If you have NLTK installed, CrazyTokenizer can use PorterStemmer or
WordNetLemmatizer for you. Just pass <code class="docutils literal notranslate"><span class="pre">stem</span></code> or <code class="docutils literal notranslate"><span class="pre">lemm</span></code> options
respectively to <code class="docutils literal notranslate"><span class="pre">stem</span></code> parameter.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">stem</span><span class="o">=</span><span class="s1">&#39;stem&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;I am an unbelievably fantastic human being&quot;</span><span class="p">)</span>
<span class="go">[&#39;i&#39;, &#39;am&#39;, &#39;an&#39;, &#39;unbeliev&#39;, &#39;fantast&#39;, &#39;human&#39;, &#39;be&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="removing-punctuation-and-linebreaks">
<h3>Removing punctuation and linebreaks<a class="headerlink" href="#removing-punctuation-and-linebreaks" title="Permalink to this headline">¬∂</a></h3>
<p>Punctuation and linebreak characters usually just introduce extra noise
to your text classification problem,
so you can easily remove it with <code class="docutils literal notranslate"><span class="pre">removepunct</span></code> and <code class="docutils literal notranslate"><span class="pre">removebreaks</span></code> options.
Both default to True.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">removepunct</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">removebreaks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;I love my life, friends, and oxford commas. </span><span class="se">\n</span><span class="s2"> Amen!&quot;</span><span class="p">)</span>
<span class="go">[&#39;i&#39;, &#39;love&#39;, &#39;my&#39;, &#39;life&#39;, &#39;friends&#39;, &#39;and&#39;, &#39;oxford&#39;, &#39;commas&#39;, &#39;amen&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="decontracting">
<h3>Decontracting<a class="headerlink" href="#decontracting" title="Permalink to this headline">¬∂</a></h3>
<p>CrazyTokenizer can attempt to expand some of those annoying contractions
for you. <strong>Note</strong>: use at your own risk.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">decontract</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;I&#39;ll have two number nines, a number nine large...&quot;</span><span class="p">)</span>
<span class="go">[&#39;i&#39;, &#39;will&#39;, &#39;have&#39;, &#39;two&#39;, &#39;number&#39;, &#39;nines&#39;, &#39;a&#39;, &#39;number&#39;, &#39;nine&#39;, &#39;large&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="dealing-with-hashtags">
<h3>Dealing with hashtags<a class="headerlink" href="#dealing-with-hashtags" title="Permalink to this headline">¬∂</a></h3>
<p>Hashtags are super-popular on Twitter. CrazyTokenizer can do one of
three things about them:</p>
<blockquote>
<div><ul class="simple">
<li>Do nothing (<code class="docutils literal notranslate"><span class="pre">hashtags=False,</span> <span class="pre">splithashtags=False</span></code>)</li>
<li>Replace all of them with a placeholder token (<code class="docutils literal notranslate"><span class="pre">hashtags='TOKEN'</span></code>)</li>
<li>Split them into separate words (<code class="docutils literal notranslate"><span class="pre">hashtags=False,</span> <span class="pre">splithashtags=True</span></code>)</li>
</ul>
</div></blockquote>
<p>Splitting hashtags is especially useful for the Reddit-based models since
hashtags are not used on Reddit, and you can potentially lose a lot of semantic
information when you calculate RedditScores for the Twitter data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">hashtags</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">splithashtags</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Let&#39;s #makeamericagreatagain#americafirst&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&quot;let&#39;s&quot;, &quot;#makeamericagreatagain&quot;, &quot;#americafirst&quot;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">hashtags</span><span class="o">=</span><span class="s2">&quot;HASHTAG_TOKEN&quot;</span><span class="p">,</span> <span class="n">splithashtags</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">[&quot;let&#39;s&quot;, &quot;HASHTAG_TOKEN&quot;, &quot;HASHTAG_TOKEN&quot;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">hashtags</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">splithashtags</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&quot;let&#39;s&quot;, &quot;make&quot;, &quot;america&quot;, &quot;great&quot;, &quot;again&quot;, &quot;america&quot;, &quot;first&quot;]</span>
</pre></div>
</div>
</div>
<div class="section" id="dealing-with-special-tokens">
<h3>Dealing with special tokens<a class="headerlink" href="#dealing-with-special-tokens" title="Permalink to this headline">¬∂</a></h3>
<p>CrazyTokenizer correctly handles Twitter handles, subreddits, Reddit usernames,
emails, all sorts of numbers, and extracts them as separate tokens:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;@crazyfrogspb recommends /r/BeardAdvice!&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;@crazyfrogspb&#39;, &#39;recommends&#39;, &#39;/r/beardadvice&#39;]</span>
</pre></div>
</div>
<p>However, you might want to completely remove certain types of tokens
(for example, it makes sense to remove subreddit names if you want to compute
RedditScores for the Twitter data), or to replace them with special tokens.
Well, it‚Äôs your lucky day, CrazyTokenizer can do that!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">subreddits</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">twitter_handles</span><span class="o">=</span><span class="s1">&#39;ANOTHER_TWITTER_USER&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;ANOTHER_TWITTER_USER&#39;, &#39;recommends&#39;]</span>
</pre></div>
</div>
<p>There is a special option for Twitter handles: ‚Äòrealname‚Äô. It replaces each
handle with the screen name of the user that is listed in their profile.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">splithashtags</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">twitter_handles</span><span class="o">=</span><span class="s1">&#39;realname&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;@realDonaldTrump please #MakeAmericaGreatAgain&#39;</span><span class="p">)</span>
<span class="go">[&#39;donald&#39;, &#39;j.&#39;, &#39;trump&#39;, &#39;please&#39;, &#39;make&#39;, &#39;america&#39;, &#39;great&#39;, &#39;again&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="urls">
<h3>URLs<a class="headerlink" href="#urls" title="Permalink to this headline">¬∂</a></h3>
<p>NLP practicioners often simply remove all URL occurrences since they do not
seem to contain any useful semantic information. Of course, CrazyTokenizer
correctly recognizes URLs as separate tokens and can remove or replace them
with a placeholder token.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Where is my job then?https://t.co/pN2TE5HDQm&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;where&#39;, &#39;is&#39;, &#39;my&#39;, &#39;job&#39;, &#39;then&#39;, &#39;https://t.co/pN2TE5HDQm&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="s1">&#39;URL&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;where&#39;, &#39;is&#39;, &#39;my&#39;, &#39;job&#39;, &#39;then&#39;, &#39;URL&#39;]</span>
</pre></div>
</div>
<p>CrazyTokenizer can do something even more interesting though. Let‚Äôs explore
all options one by one.</p>
<p>First, CrazyTokenizer can extract domains from your URLs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="s1">&#39;domain&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;http://nytimes.com or http://breitbart.com, that is the question&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;nytimes&#39;, &#39;or&#39;, &#39;breitbart&#39;, &#39;that&#39;, &#39;is&#39;, &#39;the&#39;, &#39;question&#39;]</span>
</pre></div>
</div>
<p>Unfortunately, links on Twitter are often shortened, so extracting domain
directly doesn‚Äôt make a lot of sense. Not to worry though, CrazyTokenizer
can handle that for you! Setting <code class="docutils literal notranslate"><span class="pre">urls='domain_unwrap_fast'</span></code> will deal with
links shortened by the following URL shorteners:
t.co, bit.ly, goo.gl, tinyurl.com.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="s1">&#39;domain_unwrap_fast&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Where is my job then?https://t.co/pN2TE5HDQm&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;where&#39;, &#39;is&#39;, &#39;my&#39;, &#39;job&#39;, &#39;then&#39;, &#39;bloomberg_domain&#39;]</span>
</pre></div>
</div>
<p>If you want, CrazyTokenizer can attempt to unwrap ALL extracted URLs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="s1">&#39;domain_unwrap&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Where is my job then?https://t.co/pN2TE5HDQm&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;where&#39;, &#39;is&#39;, &#39;my&#39;, &#39;job&#39;, &#39;then&#39;, &#39;bloomberg_domain&#39;]</span>
</pre></div>
</div>
<p>Last but not least, CrazyTokenizer can extract web page titles, tokenize them,
and insert to your tokenized sentences. Note: it won‚Äôt extract titles from the
Twitter pages in order to avoid duplicating tweets content.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="s1">&#39;title&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;I love Russia https://goo.gl/3ioXU4&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;i&#39;, &#39;love&#39;, &#39;russia&#39;, &#39;russia&#39;, &#39;to&#39;, &#39;block&#39;, &#39;telegram&#39;, &#39;app&#39;, &#39;over&#39;, &#39;encryption&#39;, &#39;bbc&#39;, &#39;news&#39;]</span>
</pre></div>
</div>
<p><strong>Please note</strong> that CrazyTokenizer has to make requests to the websites,
and it is a very time-consuming operation, so CrazyTokenizer saves all
parsed domains and web page titles. If you plan to experiment with
the different preprocessing options and/or models, you should consider saving
extracted domains/titles and then supplying saved dictionary as an argument
to <code class="docutils literal notranslate"><span class="pre">urls</span></code> parameter.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">json</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;domains.json&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="go">      json.dump(tokenizer._domains, f)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;realnames.json&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="go">      json.dump(tokenizer._realnames, f)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;domains.json&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="go">      domains = json.load(f)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;realnames.json&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="go">      realnames = json.load(f)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">domains</span><span class="p">,</span> <span class="n">twitter_handles</span><span class="o">=</span><span class="n">realnames</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="extra-patterns-and-keeping-untokenized">
<h3>Extra patterns and keeping untokenized<a class="headerlink" href="#extra-patterns-and-keeping-untokenized" title="Permalink to this headline">¬∂</a></h3>
<p>You can also supply your own replacement rules to CrazyTokenizer. In particular,
you need to provide a tuple that contains unique name for your rule, compiled
re pattern and a replacement token.</p>
<p>Also, it makes sense to keep some common expressions (e.g., ‚ÄúNew York Times‚Äù)
untokenized. If you think that it can improve your model quality, feel free to
supply a list of strings that should be kept as single tokens.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">re</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rule0</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[S,s]ucks&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rule1</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[R,r]ules&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">extra_patterns</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;rule0&#39;</span><span class="p">,</span> <span class="n">rule0</span><span class="p">,</span> <span class="s1">&#39;rules&#39;</span><span class="p">),</span>
<span class="go">                                               (&#39;rule1&#39;, &#39;rule1, &quot;sucks&#39;)],</span>
<span class="go">                               keep_untokenized=[&#39;St.Petersburg&#39;],</span>
<span class="go">                               lowercase=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Moscow rules, St.Petersburg sucks&quot;</span>
<span class="go">[&#39;Moscow&#39;, &#39;sucks&#39;, &#39;St.Petersburg&#39;, &#39;rules&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="converting-whitespaces-to-underscores">
<h3>Converting whitespaces to underscores<a class="headerlink" href="#converting-whitespaces-to-underscores" title="Permalink to this headline">¬∂</a></h3>
<p>Popular implementations of models (most notably, fastText) do not support
custom token splitting rules and simply split on whitespaces. In order to deal
with that, CrazyTokenizer can replace all whitespaces in the final tokens by
underscores (enabled by deafult).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">whitespaces_to_underscores</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">keep_untokenized</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;New York&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;New York is a great place to make a rat friend&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;new_york&#39;, &#39;is&#39;, &#39;a&#39;, &#39;great&#39;, &#39;place&#39;, &#39;to&#39;, &#39;make&#39;, &#39;a&#39;, &#39;rat&#39;, &#39;friend&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="removing-non-unicode-characters">
<h3>Removing non-unicode characters<a class="headerlink" href="#removing-non-unicode-characters" title="Permalink to this headline">¬∂</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">remove_nonunicode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;–†–æ—Å—Å–∏—è - —Å–≤—è—â–µ–Ω–Ω–∞—è –Ω–∞—à–∞ –¥–µ—Ä–∂–∞–≤–∞, –†–æ—Å—Å–∏—è - –≤–µ–ª–∏–∫–∞—è –Ω–∞—à–∞ —Å—Ç—Ä–∞–Ω–∞!&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
</div>
<div class="section" id="emojis">
<h3>Emojis<a class="headerlink" href="#emojis" title="Permalink to this headline">¬∂</a></h3>
<p>Social media users are notoriously famous for their excessive use of emojis.
CrazyTokenizer correctly separates consecutive emojis.</p>
<p>In addition, CrazyTokenizer can replace different kind of emojis with the
corresponding word tokens.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">pos_emojis</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">neg_emojis</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">neutral_emojis</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;üòçüò≠üò©???!!!!&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;POS_EMOJI&#39;, &#39;NEG_EMOJI&#39;, &#39;NEG_EMOJI&#39;]</span>
</pre></div>
</div>
<p>You can supply your own lists of emojis as well.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">pos_emojis</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;üåÆ&#39;</span><span class="p">,</span> <span class="s1">&#39;üçî&#39;</span><span class="p">],</span> <span class="n">neutral_emojis</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;üòï&#39;</span><span class="p">],</span> <span class="n">removepunct</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;üåÆ + üçî = üòï&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="go">[&#39;POS_EMOJI&#39;, &#39;+&#39;, &#39;POS_EMOJI&#39;, &#39;=&#39;, &#39;NEUTRAL_EMOJI&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="unicode-and-hex-characters">
<h3>Unicode and hex characters<a class="headerlink" href="#unicode-and-hex-characters" title="Permalink to this headline">¬∂</a></h3>
<p>Sometimes your data gets messed up as a result of repeated save/load operations.
If your data contains a lot of substrings that look like this: <code class="docutils literal notranslate"><span class="pre">\\xe2\\x80\\x99</span></code>
or this: <code class="docutils literal notranslate"><span class="pre">U+1F601</span></code>, try setting <code class="docutils literal notranslate"><span class="pre">latin_chars_fix=True</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CrazyTokenizer</span><span class="p">(</span><span class="n">latin_chars_fix</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;I</span><span class="se">\\</span><span class="s2">xe2</span><span class="se">\\</span><span class="s2">x80</span><span class="se">\\</span><span class="s2">x99m so annoyed by these characters </span><span class="se">\\</span><span class="s2">xF0</span><span class="se">\\</span><span class="s2">x9F</span><span class="se">\\</span><span class="s2">x98</span><span class="se">\\</span><span class="s2">xA2&quot;</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="modelling.html" class="btn btn-neutral float-right" title="Modelling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="data_collection.html" class="btn btn-neutral" title="Data collection" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Evgenii Nikitin.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.4.0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script> 

</body>
</html>